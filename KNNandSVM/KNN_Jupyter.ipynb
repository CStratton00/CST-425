{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the data\n",
    "\n",
    "# The column names are not included in the .data file so we must create an array of the column names and add them\n",
    "colnames = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', '50k-Prediction']\n",
    "\n",
    "# Create a panda dataframe from adult.data and add column names\n",
    "df = pd.read_csv(\"adult.data\", names=colnames)\n",
    "\n",
    "# Print out the top 20 instances from the dataframe\n",
    "print(df.head(20))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Remove duplicate data rows\n",
    "df.drop_duplicates()\n",
    "\n",
    "# Label Encoder is used to take all of our string data and asssign it a unique integer value\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Using .fit_transform function to fit label\n",
    "# encoder and return encoded label\n",
    "# example, male/female will be turned into 0 and 1 for knn to calculate distance from\n",
    "df['workclass'] = le.fit_transform(df['workclass'])\n",
    "df['education'] = le.fit_transform(df['education'])\n",
    "df['marital-status'] = le.fit_transform(df['marital-status'])\n",
    "df['occupation'] = le.fit_transform(df['occupation'])\n",
    "df['relationship'] = le.fit_transform(df['relationship'])\n",
    "df['race'] = le.fit_transform(df['race'])\n",
    "df['sex'] = le.fit_transform(df['sex'])\n",
    "df['native-country'] = le.fit_transform(df['native-country'])\n",
    "df['50k-Prediction'] = le.fit_transform(df['50k-Prediction'])  # Under 50k = 0, Over 50k = 1\n",
    "\n",
    "# Print out the top 20 instances from the dataframe to show how LabelEncoder changed our data\n",
    "print(df.head(20))\n",
    "\n",
    "# Uncomment the .iloc[] lines if the program is running too slow. Will reduce dataset by 2/3.\n",
    "x = df.drop('50k-Prediction', axis=1)   # all columns except 50k Prediction column (or classifier)\n",
    "x10000 = x.iloc[:10000,].values              # Set sample amount to first 10000 rows\n",
    "\n",
    "y = df['50k-Prediction']                # only the 50k Prediction column (or classifier),\n",
    "y10000 = y.iloc[:10000,].values              # Set sample amount to first 10000 rows\n",
    "\n",
    "# Need to normalize our data so 0/1 values are weighted the same as 0-50 values\n",
    "# Dont need to normalize our y data because it is only 0s and 1s.\n",
    "print(type(x))\n",
    "x = preprocessing.scale(x)\n",
    "print(type(x))\n",
    "x = pd.DataFrame(x)\n",
    "print(type(x))\n",
    "x10000 = preprocessing.scale(x10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Because KNN is a supervised classifier, split dataset into training set and test set\n",
    "# The X_train and y_train variables will be the 70% of the data, while X_test and y_test will be the 30%.\n",
    "# X_train and y_train will create the model, then X_test will be plugged into the model which will produce the prediction for y_test values.\n",
    "# Then we will compare the y_prediction values to the y_test values to determine the accuracy.\n",
    "# We use x10000 and y10000 becuase these variables will be used 30 times to calculate the optimal neighbor value, hence we need a smaller training set so it can runn efficiently\n",
    "X_train, X_test, y_train, y_test = train_test_split(x10000, y10000, test_size=0.3,random_state=1234)  # 70% training and 30% test\n",
    "\n",
    "\n",
    "# Determine the optimal amount of clusters using Error graph\n",
    "# The biggest bend in the elbow determines which number of neighbors creates greatest difference in error reduction\n",
    "Error = []  # will keep track of Error percentage for each n value\n",
    "for n in range(1,31):  # calculate 30 different error values\n",
    "    knnData = KNeighborsClassifier(n_neighbors=n)  # Uses KNeighborsClassifier to\n",
    "    knnData = knnData.fit(X_train,y_train)  # Use train data to create a model\n",
    "    y_pred = knnData.predict(X_test)  # Predict the y values with x_test values\n",
    "    Error.append(1-accuracy_score(y_test,y_pred))  # Compare the y_pred values to the y_test actual values to find Error\n",
    "\n",
    "plt.plot(range(1,31),Error, marker='o') # Plot the 30 different error calculations\n",
    "plt.title(\"Using KNeighborsClassifier with neighbor values 1-31\")\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "# will print the index/n_neighbor value where the error is the lowest. Each time the data is randomly selected, so it will change each time it is run\n",
    "best_n = Error.index(min(Error))+1\n",
    "print(f\"Lowest Error is with n neighbor value: {best_n}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Main KNN Calculation:\n",
    "\n",
    "# Show that x and y are full data set -> 32k instances\n",
    "print(f\"Length of x data = {len(x)}\")\n",
    "print(f\"Length of y data = {len(y)}\")\n",
    "\n",
    "# Using Kfolding to create a better estimate by averaging 5 testing and training sets\n",
    "k = 5  # Do 5 folds, and split the set into 80/20\n",
    "kfold = KFold(n_splits=k, random_state=1234, shuffle=True)\n",
    "knnclassifier = KNeighborsClassifier(n_neighbors=best_n) # Note the use of best_n\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "# For loop that will calculate the KNeighbors Classifier 5 times using 5 different folds of the training and testing\n",
    "# data, then average the results.\n",
    "for train_index, test_index in kfold.split(x):\n",
    "     x_train, x_test = x.iloc[train_index,:], x.iloc[test_index,:]\n",
    "     y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "     knnclassifier.fit(x_train,y_train)\n",
    "     y_pred = knnclassifier.predict(x_test)\n",
    "\n",
    "     acc = accuracy_score(y_pred, y_test)\n",
    "     acc_score.append(acc)\n",
    "\n",
    "avg_acc_score = sum(acc_score)/k\n",
    "\n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a confusion matrix of the y_test values(correct answers) to the y_pred values(predicted values)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "# Results from confusion matrix:\n",
    "print(\"6922 true positives\")\n",
    "print(\"1158 true negatives\")\n",
    "print(\"547 false negatives\")\n",
    "print(\"1142 false positives\")\n",
    "\n",
    "\n",
    "# Precision-> True Positives / (True Positives + False Positives)\n",
    "# How correct was the prediction?\n",
    "# 88% salary under 50k\n",
    "# 67% salary over 50k\n",
    "# Averaging the two, the chance to identify is 78%, with a weighted 83% chance\n",
    "\n",
    "# Recall-> True Positives / (True Positives + False Negatives)\n",
    "# A measure of the models completeness. How many positive cases were found?\n",
    "# 91% of cases were found if salary is less than 50k\n",
    "# 58% of cases were found if salary is over 50k\n",
    "# 75% average cases found with a weighted 84% rate\n",
    "\n",
    "# f1-score- F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "# What percent of positive predictions were correct?\n",
    "# 89% for salary <50k\n",
    "# 62% for salary > 50k\n",
    "# Accuracy of 84% to determine if a person has a salary over/under 50k\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "#Resources Used\n",
    "# https://muthu.co/understanding-the-classification-report-in-sklearn/\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}