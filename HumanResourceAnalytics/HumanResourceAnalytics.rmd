---
title: "Data Mining - Clustering"
author: "Andrew Esch, Evan Lee, and Collin Stratton"
date: "2-27-2022"
output: html_notebook
---

# Introduction
The purpose of this assignment is to be able to identify important predictors of performance within data in relation to employeesâ€™ satisfaction and company policies for the workers. This project will use a provided dataset and computational algorithms to analyze the information from the dataset and find what factors in the company policies provide the best employee satisfaction.
Principle Component Analysis, or PCA, is a technique for reducing the dimensionality of datasets and increasing the interpretability while minimizing information loss. PCA is useful when there is data on a large number of variables, and there is some redundancy in those variables. In this case, redundancy means that some of the variables are correlated with one another. And because of this redundancy PCA can be used to reduce the observed variables into a smaller number of principal components that will account for most of the variance in the observed variables. Thus, PCA is recommended as an exploratory tool to uncover unknown trends in the data.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Implementation Approach


# Human Resource Analytics
## Load PCA related packages
```{r Import Libraries}
library(psych)
library(rela)   # contains Bartlett and KMO tests to verify assumptions
library(MASS)
library(parallel)
library(caTools)
require(graphics)

set.seed(123)

options(scipen=999)
```



## Import Data
```{r Import Data}
fileName <- "/Users/collinstratton/Google Drive/School/GCU/Fall 2021 - Spring 2022/CST-425/CST-425/HumanResourceAnalytics/WA_Fn-UseC_-HR-Employee-Attrition.csv"
#fileName <- "Andrew's Path"
# fileName <- "C:/GCU Academics/Junior Year/Second Semester/CST-425/CLC Shared GitHub Repository/Cloned Repository/HumanResourceAnalytics/WA_Fn-UseC_-HR-Employee-Attrition.csv"
data <- read.csv(file=fileName, header=TRUE,sep=",", stringsAsFactors=TRUE)
head(data)
```

## Change all string factor data into numerical data for PCA
```{r Make Numerical Data}
# columns with string factors
stringFactors <- c(2,3,5,8,12,16,18,22,23)

# loop through string columns and convert to numeric
for(x in stringFactors){
    print(x)
    columndatatest <- data[,x]
    factors <- factor(columndatatest)
    data[,x] <- as.numeric(factors)
}

head(data)
```

## Normalize data for PCA
By normalizing the data it increases the cohesion and quality of the data effectively cleaning it and increasing the quality.
```{r Normalize Data}
data <- as.data.frame(scale(data))
head(data)
```

## Find and get rid of NA values
```{r Remove NA values}
# summarize data before removing NA values
sum(is.na(data))

# remove NA values
data <- subset(data, select = -c(9,22,27))

# summarize data after removing NA values
sum(is.na(data))
head(data)
```

## Linear Regression
### Training and Testing Set
Split the data into a training set and testing set, using 70% of the data for training and 30% for testing.
```{r Create Training and Testing Set}
sample <- sample.split(data$HourlyRate, SplitRatio = 0.7)

train <- subset(data, sample==TRUE)
test <- subset(data, sample==FALSE)

# Display training dataframe
head(train)
```

### We can change the variable HourlyRate to any variable we want to check out. Just change it above in the testing and training chunk as well.
```{r buildModel}
model <- lm(HourlyRate ~., data = train)
summary(model)
```
From this analysis we can see that Job Satisfaction is the most important predictor of employee attrition. Other factors that are important are Job Level, Job Involvement, and Environment Satisfaction. These will be the factors focused on in the following sections.

## Improve the Model using only Significant Variables
### We used this for our Linear Regression before, but now we can use PCA instead of pvalues
Using the P Value of the model gained from the previous section, the next step is to determine which variables are significant and use the training set to build a model with only significant variables. The code section below gives a summary of the significant variables found.
```{r Removing variables using p-values}
# Choose only variables with p-value < 2.2e-16
#highModel.significantVars <- lm(High ~ Open + Close + Volume, data = txn.df.train)
#highModel.significantVars
#summary(highModel.significantVars)
```

## PCA
As talked about before, PCA is a technique for reducing the dimensionality of datasets and increasing the interpretability while minimizing information loss. These next sections will be focused on the application of the PCA technique to the data.
```{r Principal Components and Summary}
PrincipalComponents <- princomp(data, cor = TRUE)
summary(PrincipalComponents)
```


## Display PCA Data
```{r Loadings Data}
loadings(PrincipalComponents)
```

## Graph PCA Data
```{r Plotting Principal Components}
plot(PrincipalComponents)
biplot(PrincipalComponents)
```

## Perform PCA Correlation Test
```{r PCA Correlation Test}
pcacor <- cor(data)
pcacor
summary(cor(data))
```

### Find the 3 PCA variables that are most correlated with the target variable
```{r PCA Compenent Values}
pca3Components <- principal(pcacor, nfactors=3, rotate="none") # we calculate all 3 components
pca3Components
```

### Find MR values for each PCA component
```{r}
test <- fa(pcacor, nfactors=3, rotate="none")
test
```

### Plot Parallel Analysis Plots
```{r}
alpha(pcacor)
fa.parallel(data,n.obs=161, fm="pa", fa="pc")
fa.diagram(pca3Components)
```

# Conclusion


# References
https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202
