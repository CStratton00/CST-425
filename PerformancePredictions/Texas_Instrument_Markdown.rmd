---
title: "Texas Instruments Stock Performance"
author: "Andrew Esch, Evan Lee, Collin Stratton"
date: "1/16/2022"
output: html_document
---
# Introduction

## Data Setup and exploration
```{r Import Data}
txn <- read.csv(file='PerformancePredictions/TXN.csv', header=TRUE, sep=',')
txn.df <- data.frame(txn)
head(txn.df)

# Check for missing values
sum(is.na(txn.df))

# Find data correlation
cor(txn.df[2:7])
```
```{r}
library(pracma)
xtime <- linspace(1,1259, n=1259)
highy <- txn.df[,"High"]

plot(xtime,highy)
abline(lm(highy~xtime))
```

## Training and Testing Sets Preparation
```{r Split Sets}
library(caTools)
set.seed(123)  # to reproduce the sample

# Split training and testing based on "High" variable
sample <- sample.split(txn.df$High, SplitRatio = 0.7) # Split using 70/30 ratio (backed up with research)

# Create training/testing csv files
txn.df.train <- subset(txn.df, sample==TRUE)
txn.df.test <- subset(txn.df, sample==FALSE)

# Display training dataframe
head(txn.df.train)
tail(txn.df.train)
```

## Test Set Predictions (Build the model)
```{r model}

highModel <- lm(High ~., data = txn.df.train[2:7]) # Build model based on
summary(highModel)
```

## Improve the model using only significant variables

```{r improved_model}
# Choose only variables with p-value < 2.2e-16

highModel.significantVars <- lm(High ~ Open + Close + Volume, data = txn.df.train)
highModel.significantVars

summary(highModel.significantVars)
```

## Regression output intepretation

For example, for every unit of **volatile.acidity** the quality decreases by 1.97 units, or for every increase in a unit of **sulphates** the quality increases by 0.746 units
```{r interpretation}
# The variables used in the model
names(highModel.significantVars)

# The number of fitted values in the model
length(highModel.significantVars$fitted.values)

```

### Calculate residuals
```{r Fitted Values and Residuals}

#Calculate the predicted values using the highmodel fitted values
predicted.train <- highModel.significantVars$fitted.values

#print out first 6 figures of predicted training model
head(predicted.train)

#make predicted training model a data frame
predicted.train.df <- data.frame(predicted.train)

# Calculate residual values
predicted.train.df.residuals <- highModel.significantVars$residuals
head(predicted.train.df.residuals)
```


### Make predictions using the test set
It appears that the model for the predicted values are greater than the test model, which means the model overvalued each entry.
```{r predictions}

predicted.test <- predict(highModel.significantVars, newdata = txn.df.test)
predicted.test.df <- data.frame(predicted.test)

# Plot actual values vs predicted values

plot(txn.df.test$High, col="green", type="l", lty=1, lwd=5, main = "Actual vs Predicted Values") # Plot actu
lines(predicted.test.df, col="red", type="l", lty=1, lwd=2.5)
```



## Verify Model
There's a lot of steps to verify the model (definitely more than 4)
1. Confirm linearity
2. Confirm normality
3. Check for Homoscedasticity
4. Confirm if there are significant outliers that affect the model
5. Verify that the residuals are independent
6. Verify Homoscedasticity
   A. NCV Test
7. Verify colinearity

### Step 1: Confirm linearity
```{r confirm_linearity}
plot(highModel.significantVars, which=1)
```

Explanation: To confirm linearity, check the graph for non-linear patterns. Although the data shows that there are several residual outliers, the data altogether forms a clear linear pattern. In addition, the residuals appear to be spread-out. Thus, the residuals vs. fitted graph shows that linearity can be assumed.

### Step 2: Confirm normality
```{r confirm_normality}

# Verify that the residuals are normally distributed
plot(highModel.significantVars, which=2)

```
Explanation: To confirm normality, it must be verified that the standardized residuals vs. the theoretical quartiles form a normal distribution. Although the data appears to curve away from the line at the borders of the graph, the plot shows a clear straight line. Consequently, normality is normality distributed and the normality assumption is satisfied.

### Step 3: Check for Homoscedasticity
```{r confirm_homoscedasticity}
# Verify that the residuals are randomly spread
plot(highModel.significantVars, which=3)
```
Explanation: There are three outliers: 736, 1056, and 1232. However, the data appears to form a linear pattern and are randomly-spread. Thus, we can confirm that homoscedasticity assumption is verified.

### Step 4: Verify if there are any outliers that could skew results
```{r verification_outliers}

# Assess the presence of significant outliers that could skew the results
plot(highModel.significantVars, which=4)
plot(highModel.significantVars, which=5)
```
Explanation: Using the first graph to verify the cook's distance vs. observation number, there are 3 outliers that appear: 799, 800, 1059. However, the general average cook's distance vs. observation number appears to stay around 0.0. To confirm that these are outliers, graph #2 demonstrates that the same observation numbers also appear as outliers in the standardized residuals vs. leverage plot. Although there appears to be several data points that stray away from the main cluster of data, all data fits within the cook's distance lines. Thus, the graphs affirm that there are no significant outliers that significantly affect the model.

### Step 5: Verify that there is no autocorrelation
```{r confirm_no_autocorrelation}

# Verify that the residuals are independent (i.e. not auto-correlated)

library(car) # companion to applied regression

# Test for autocorrelation using the Durbin-Watson Test
durbinWatsonTest(highModel.significantVars)
```
Explanation: To confirm that the residuals are independent, the Durbin-Watson test is one method to determine if an autocorrelation exists.

Autocorrelation is the correlation measure of a lagged time series in comparison to a current time series. In other words... To avoid a bias in the linear regression prediction model, there cannot be statistical evidence of an autocorrelation. The null hypothesis is that there is no autocorration. The alternative hypothesis would state that there is an autocorrelation. For this model, use the standard that the p-value < 0.001. Using the Durbin-Watson test, the test-statistic and p-value and insigificant (D-W statistic = 1.45, p-value = 0). Thus, we fail to reject the null hypothesis. This means that there is no statistical evidence that the residuals are not independent and the assumption is satisfied.

### Step 6: Verify that there is no colinearity
```{r verification_colinearity}
# Test Collinearity using Variance Inflation Factor (VIF)
vif(highModel.significantVars)

# Test that SQRT(VIF) > 5
sqrt(vif(highModel.significantVars)) > 5
```
Explanation: Reviewing the vif outputs for the three significant variables, the "Open" and "Close" variables show a significantly high VIF value (vif = 425). Consequently, these variables has a sqrt(vif) > 5 and failed the collinarity test. This means that the variables "Open" and "Close" are correlated to each other.

Note: Write more about what this means for our model???


### Final financial "High" prediction results
```{r final_predictions}

predicted.test <- predict(highModel.significantVars, newdata = txn.df.test)
predicted.test.df <- data.frame(predicted.test)
head(predicted.test.df[order(predicted.test.df$predicted.test),], 20)


```
## References
