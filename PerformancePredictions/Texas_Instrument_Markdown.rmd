---
title: "Texas Instruments Stock Performance"
author: "Andrew Esch, Evan Lee, Collin Stratton"
date: "1/16/2022"
output: word_document
---
# Introduction
The purpose of this assignment is to take the data from the Texas Instruments stock and, given its performance characteristics and investment objectives, predict its future performance using linear modeling. This project also serves as an introduction to R and linear modeling.

## Data Setup and exploration
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Import the data and set the dataframe as txn.
```{r Import Data}
# If you need to knit this, use your full directory path (with forward-slashes) like this: C:/Users/andre/Documents/GitHub/CST-425/PerformancePredictions/TXN.csv
txn <- read.csv(file='C:/Users/Evan/OneDrive/Documents/GitHub/CST-425/PerformancePredictions/TXN.csv', header=TRUE, sep=',')
txn.df <- data.frame(txn)
head(txn.df)

# Check for missing values
sum(is.na(txn.df))

# Find data correlation
cor(txn.df[2:7])
```
Use the pracma library to plot the data from Texas Instruments. The graph below depicts the price of the stock at its highest over time. Furthermore, abline is used to draw a line to create a linear model predicting the price of the stock.
```{r}
library(pracma)
# create an array x that contatins time equal to the recorded high prices
xtime <- linspace(1,1259, n=1259)
highy <- txn.df[,"High"]

# plot time vs high price and add a line to create a linear model
plot(xtime,highy)
abline(lm(highy~xtime))
```

From the graph it is shown that Texas Instruments has performed well over the time frame selected. The linear plot of the data predicts the price of the stock will continue to increase over time. From here, more tests will be performed to determine if the linear plot is accurate.

## Training and Testing Sets Preparation
Install caTools and use the function sample to split the data into training and testing sets. The head of the training set is shown below.
```{r Split Sets}
library(caTools)
set.seed(123)  # to reproduce the sample

# Split training and testing based on "High" variable
sample <- sample.split(txn.df$High, SplitRatio = 0.7) # Split using 70/30 ratio (backed up with research)

# Create training/testing csv files
txn.df.train <- subset(txn.df, sample==TRUE)
txn.df.test <- subset(txn.df, sample==FALSE)

# Display training dataframe
head(txn.df.train)
```

## Test Set Predictions (Build the model)
Build a model using the training set for the highest price of the stock over the given time frame. A summary model is shown below.
```{r model}
# Build model using training set
highModel <- lm(High ~., data = txn.df.train[2:7])
summary(highModel)
```

## Improve the Model using only Significant Variables
Using the P Value of the model gained from the previous section, the next step is to determine which variables are significant and use the training set to build a model with only significant variables. The code section below gives a summary of the significant variables found.
```{r improved_model}
# Choose only variables with p-value < 2.2e-16
highModel.significantVars <- lm(High ~ Open + Close + Volume, data = txn.df.train)
highModel.significantVars

summary(highModel.significantVars)
```

## Regression Output Interpretation
After finding each significant variable, the next step is to determine the regression output interpretation by determining the number of fitted values that best fit the TXN model.
```{r interpretation}
# The variables used in the model
names(highModel.significantVars)

# The number of fitted values in the model
length(highModel.significantVars$fitted.values)
```

### Calculate residuals
The residuals are calculated using the fitted values gathered from the significant variables. The residuals are shown below.
```{r Fitted Values and Residuals}
#Calculate the predicted values using the highmodel fitted values
predicted.train <- highModel.significantVars$fitted.values

#print out first 6 figures of predicted training model
head(predicted.train)

#make predicted training model a data frame
predicted.train.df <- data.frame(predicted.train)

# Calculate residual values
predicted.train.df.residuals <- highModel.significantVars$residuals
head(predicted.train.df.residuals)
```

### Make predictions using the test set
It appears that the model for the predicted values are greater than the test model, which means the model overvalued each entry.
```{r predictions}
predicted.test <- predict(highModel.significantVars, newdata = txn.df.test)
predicted.test.df <- data.frame(predicted.test)

# Plot actual values vs predicted values
plot(txn.df.test$High, col="green", type="l", lty=1, lwd=5, main = "Actual vs Predicted Values")
lines(predicted.test.df, col="red", type="l", lty=1, lwd=2.5)
```

## Verify Model
There's a lot of steps to verify the model (definitely more than 4)
1. Confirm linearity
2. Confirm normality
3. Check for Homoscedasticity
4. Confirm if there are significant outliers that affect the model
5. Verify that the residuals are independent
6. Verify Homoscedasticity
   A. NCV Test (not required unless there is a pattern in verifying Homoscedasticity)
7. Verify colinearity

### Step 1: Confirm linearity
```{r confirm_linearity}
plot(highModel.significantVars, which=1)
```

Explanation: To confirm linearity, check the graph for non-linear patterns. Although the data shows that there are several residual outliers, the data altogether forms a clear linear pattern. In addition, the residuals appear to be spread-out. Thus, the residuals vs. fitted graph shows that linearity can be assumed.

### Step 2: Confirm normality
```{r confirm_normality}
# Verify that the residuals are normally distributed
plot(highModel.significantVars, which=2)
```

Explanation: To confirm normality, it must be verified that the standardized residuals vs. the theoretical quartiles form a normal distribution. Although the data appears to curve away from the line at the borders of the graph, the plot shows a clear straight line. Consequently, normality is normality distributed and the normality assumption is satisfied.

### Step 3: Check for Homoscedasticity
```{r confirm_homoscedasticity}
# Verify that the residuals are randomly spread
plot(highModel.significantVars, which=3)
```

Explanation: There are three outliers: 736, 1056, and 1232. However, the data appears to form a linear pattern and are randomly-spread. Thus, we can confirm that homoscedasticity assumption is verified.

### Step 4: Verify if there are any outliers that could skew results
```{r verification_outliers}
# Assess the presence of significant outliers that could skew the results
plot(highModel.significantVars, which=4)
plot(highModel.significantVars, which=5)
```

Explanation: Using the first graph to verify the cook's distance vs. observation number, there are 3 outliers that appear: 799, 800, 1059. However, the general average cook's distance vs. observation number appears to stay around 0.0. To confirm that these are outliers, graph #2 demonstrates that the same observation numbers also appear as outliers in the standardized residuals vs. leverage plot. Although there appears to be several data points that stray away from the main cluster of data, all data fits within the cook's distance lines. Thus, the graphs affirm that there are no significant outliers that significantly affect the model.

### Step 5: Verify that there is no autocorrelation
```{r confirm_no_autocorrelation}
# Verify that the residuals are independent (i.e. not auto-correlated)
library(car) # companion to applied regression

# Test for autocorrelation using the Durbin-Watson Test
durbinWatsonTest(highModel.significantVars)
```
Explanation: To confirm that the residuals are independent, the Durbin-Watson test is one method to determine if an autocorrelation exists.

Autocorrelation is the correlation measure of a lagged time series in comparison to a current time series. In other words, to avoid a bias in the linear regression prediction model, there cannot be statistical evidence of an autocorrelation. The null hypothesis is that there is no autocorration. The alternative hypothesis would state that there is an autocorrelation. For this model, use the standard that the p-value < 0.001. Using the Durbin-Watson test, the test-statistic and p-value and insigificant (D-W statistic = 1.45, p-value = 0). Thus, we fail to reject the null hypothesis. This means that there is no statistical evidence that the residuals are not independent and the assumption is satisfied.

### Step 6: Verify that there is no colinearity
```{r verification_colinearity}
# Test Collinearity using Variance Inflation Factor (VIF)
vif(highModel.significantVars)

# Test that SQRT(VIF) > 5
sqrt(vif(highModel.significantVars)) > 5
```
Explanation: Reviewing the vif outputs for the three significant variables, the "Open" and "Close" variables show a significantly high VIF value (vif = 425). Consequently, these variables has a sqrt(vif) > 5 and failed the collinarity test. This means that the variables "Open" and "Close" are correlated to each other.

### Final financial "High" prediction results
```{r final_predictions}
predicted.test <- predict(highModel.significantVars, newdata = txn.df.test)
predicted.test.df <- data.frame(predicted.test)
head(predicted.test.df[order(predicted.test.df$predicted.test),], 20)
```

## Christian Worldview
When analyzing a publicly traded stock, one must not only consider the financial values of the company, but also the moral and ethical values of the company.
This analysis done for TXN does not always mean that this is a good investment. In Matthew 25:14-30, the parable describes a master investing into three servants, each with varying levels of return. Two servant doubles the amount given, while the third does not make any. This parable is shown that knowing a company is reliable and trustworthy is important to investing as well.


## References
https://finance.yahoo.com/quote/TXN/history?period1=1484438400&period2=1642204800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true
https://padlet.com/isac_artzi/9014mn5elmn7r8sg
https://www.researchgate.net/publication/328589123_On_Splitting_Training_and_Validation_Set_A_Comparative_Study_of_Cross-Validation_Bootstrap_and_Systematic_Sampling_for_Estimating_the_Generalization_Performance_of_Supervised_Learning
