---
title: "Data Mining - Clustering"
author: "Andrew Esch, Evan Lee, and Collin Stratton"
date: "2-13-2022"
output: html_notebook
---

# Introduction
The purpose of this project is to apply data mining algorithms to perform clustering then provide a clear analysis of the data. This project uses the K-Means Clustering algorithm to classify information about online shopping intentions. Moreover, this project uses the Association Rules Mining algorithm to classify information about TK.

# Part 1
Clustering is a collection of data points aggregated into similar groups. When using k-means clustering, the user will specify the number of clusters they want to create, or rather the number of centroids. The algorithm will then assign each data point to the closest cluster using the in-cluster sum of squares. The goal of clustering is to group similar data points and discover relationships from the clusters. It is advantageous to use because it is relatively time-efficient and can easily visualize the data. Its disadvantages are that it requires centroids to be inputted prior, so it doesn't generate the most optimal number of clusters automatically, and it cannot handle noisy data and outliers. An example of clustering will be further explored in part 2.

Association Rule Mining is the process of finding patterns in data that can be used to predict the next event. If event A were to occur, how likely would event B occur? Using an example of finding the most commonly purchased grocery store items, the association is found by creating a list of all combinations of items purchased together. These combinations are used to find a confidence value for if item A is purchased, then item B will be purchased. This is very useful for places like grocery stores because they can use the data to organize their products to provide a better customer experience. Some drawbacks of association rule mining are that data collection is difficult due to the unique nature of text needed in the data, it can discover a huge number of rules or patterns that ultimately mean nothing, and the larger the dataset, the slower the algorithm. An example of association rule mining will be further explored in part 2.

# Part 2
Note: This may take some time to compile and run.

## Step 1: Get data and libraries
```{r Libraries}
# General
library(readr)
library(ggplot2)

# K-Means Clustering
library(factoextra)
library(stats)

# Association Rules Mining
library(arules)
library(arulesViz)
```

### K-Means Clustering Data
```{r getDataKMeans}
# Evan's CWD -> C:/Users/Evan/OneDrive/Documents/GitHub/CST-425/Clustering/online_shoppers_intention.csv
# Andrew's CWD -> C:/Users/andre/Documents/GitHub/CST-425/Clustering/online_shoppers_intention.csv
kmeansdf <- read.csv("C:/Users/andre/Documents/GitHub/CST-425/Clustering/online_shoppers_intention.csv")
kmeansdf <- data.frame(kmeansdf)
head(kmeansdf)
```

### Association Rules Mining Data


# K-Means

## Step 2 (K-Means): Pre-processing Data

### Setup variables and remove unnecessary columns
```{r KMeansVariables}
#df$open <- as.numeric(gsub('[$,]', '', df$open))
#df$close <- as.numeric(gsub('[$,]', '', df$close))
#df$high <- as.numeric(gsub('[$,]', '', df$high))
#df$low <- as.numeric(gsub('[$,]', '', df$low))
#df$volume <- as.numeric(gsub('[$,]', '', df$volume))

head(kmeansdf)
```

### Data Cleaning
```{r KMeansDataCleaning}
# Data cleaning
kmeansdf <- kmeansdf[,4:8]
head(kmeansdf)
kmeansdf <- na.omit(kmeansdf) # Get rid of missing values
kmeansdf <- scale(kmeansdf) # Set scale for data
head(kmeansdf, n = 3)

# Detect outliers using z-score method
z_scores <- as.data.frame(sapply(kmeansdf, function(kmeansdf) (abs(kmeansdf-mean(kmeansdf))/sd(kmeansdf))))

# Show rows in dataframe with all z-scores less than absolute value of 3
no_outliers <- z_scores[!rowSums(z_scores>3), ]
```

## Step 3 (K-Means): Build the model

### Decide number of groups using the elbow method
```{r KMeansElbowMethod}
set.seed(123)
kmeansdf <- kmeansdf[1:1000]
kmeansdf <- data.frame(kmeansdf)
# DO NOT UNCOMMENT THIS UNLESS YOU HAVE A CONFIDENT SOLUTION TO THE ELBOW METHOD THAT INVOLVES THIS
fviz_nbclust(kmeansdf, kmeans, method = "wss")
```

### Build the model using optimal number of groups and show the results
```{r KMeansModel}
kmeansmodel <- kmeans(kmeansdf, centers = 4, nstart = 25)
str(kmeansmodel)
```

## Step 4 (K-Means): Run the model and make predictions
```{r KMeansPredictions}
# TK
```

## Step 5 (K-Means): Display the results (quantitative and visual)
```{r KMeansResults}
# Quantitative Results

# Visual Results
fviz_cluster(kmeansmodel, data = kmeansdf)
```

## Steps 6-7 (K-Means): Interpret the results and adjust your clustering
Explain here TK







#------------------------------------------------------------------------------------------------------


# Association Rules Mining

```{r getDataAssociation}
# Evan's CWD -> C:/Users/Evan/OneDrive/Documents/GitHub/CST-425/Clustering/Store_Hackathon_Ideal_Data.csv
# Andrew's CWD -> C:/Users/andre/Documents/GitHub/CST-425/Clustering/Store_Hackathon_Ideal_Data.csv
associationdf <- read.csv("C:/Users/andre/Documents/GitHub/CST-425/Clustering/Store_Hackathon_Ideal_Data.csv")
associationdf <- as (associationdf, "transactions") # convert to 'transactions' class
str(associationdf)
```


## Step 2 (Association Rules Mining): Pre-processing Data

### Setup variables and remove unnecessary columns
```{r AssociationVariables}
head(associationdf)
```

### Data Cleaning
```{r AssociationDataCleaning}
#associationdf[,1] <- as.factor(associationdf[,1])
#associationdf[,2] <- as.factor(associationdf[,2])
#associationdf[,3] <- as.factor(associationdf[,3])
#associationdf[,4] <- as.factor(associationdf[,4])
#associationdf[,5] <- as.factor(associationdf[,5])
#associationdf[,6] <- as.factor(associationdf[,6])
#associationdf[,7] <- as.factor(associationdf[,7])
#associationdf[,8] <- as.factor(associationdf[,8])
#associationdf[,9] <- as.factor(associationdf[,9])
#associationdf[,10] <- as.factor(associationdf[,10])

#class(associationdf)
#head(associationdf)
```

## Step 3 (Association Rules Mining): Build the model

### Decide lift and support using the elbow method
```{r AssociationElbowMethod}
set.seed(123)
# TK
```

### Build the model using optimal variables and show the model results
```{r AssociationModel}
frequentItems <- eclat(associationdf, parameter = list(supp = 0.07, maxlen = 15)) # calculates support for frequent items
inspect(frequentItems)
itemFrequencyPlot(associationdf, topN=20, type="absolute", main="Item Frequency") # plot frequent items
```

## Step 4 (Association Rules Mining): Run the model and make predictions
```{r AssociationPredictions}
rules <- apriori(associationdf, parameter = list(supp = 0.001, conf = 0.5)) # Min Support as 0.001, confidence as 0.5.
rules_conf <- sort(rules, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf)) # show the support, lift and confidence for all rules

rules_lift <- sort(rules, by="lift", decreasing=TRUE) # 'high-lift' rules.
inspect(head(rules_lift)) # show the support, lift and confidence for all rules
```

## Step 5 (Association Rules Mining): Display the results (quantitative and visual)
```{r AssociationResults}
# Quantitative Results

# Visual Results

```

## Steps 6-7 (Association Rules Mining): Interpret the results and adjust your clustering
Explain here TK


# References
https://uc-r.github.io/kmeans_clustering#kmeans

https://www.datanovia.com/en/lessons/data-preparation-and-r-packages-for-cluster-analysis/

https://www.statology.org/remove-outliers-r/