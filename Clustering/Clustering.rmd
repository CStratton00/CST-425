---
title: "Data Mining - Clustering"
author: "Andrew Esch, Evan Lee, and Collin Stratton"
date: "2-13-2022"
output: html_notebook
---

# Introduction
The purpose of this project is to apply data mining algorithms to perform clustering then provide a clear analysis of the data. This project uses the K-Means Clustering algorithm to classify information about online shopping intentions. Moreover, this project uses the Association Rules Mining algorithm to classify information about TK.

# Part 1
Clustering is a collection of data points aggregated into similar groups. When using k-means clustering, the user will specify the number of clusters they want to create, or rather the number of centroids. The algorithm will then assign each data point to the closest cluster using the in-cluster sum of squares. The goal of clustering is to group similar data points and discover relationships from the clusters. It is advantageous to use because it is relatively time-efficient and can easily visualize the data. Its disadvantages are that it requires centroids to be inputted prior, so it doesn't generate the most optimal number of clusters automatically, and it cannot handle noisy data and outliers. An example of clustering will be further explored in part 2.

Association Rule Mining is the process of finding patterns in data that can be used to predict the next event. If event A were to occur, how likely would event B occur? Using an example of finding the most commonly purchased grocery store items, the association is found by creating a list of all combinations of items purchased together. These combinations are used to find a confidence value for if item A is purchased, then item B will be purchased. This is very useful for places like grocery stores because they can use the data to organize their products to provide a better customer experience. Some drawbacks of association rule mining are that data collection is difficult due to the unique nature of text needed in the data, it can discover a huge number of rules or patterns that ultimately mean nothing, and the larger the dataset, the slower the algorithm. An example of association rule mining will be further explored in part 2.

# Part 2
Note: This may take some time to compile and run.

## Step 1: Get data and libraries
```{r Libraries}
# General
library(readr)
library(ggplot2)

# K-Means Clustering
library(factoextra)
library(stats)

# Association Rules Mining
library(arules)
library(arulesViz)
library(dplyr)
library(DT)
```

### K-Means Clustering Data
```{r getDataKMeans}
# Evan's CWD -> C:/Users/Evan/OneDrive/Documents/GitHub/CST-425/Clustering/online_shoppers_intention.csv
# Andrew's CWD -> C:/Users/andre/Documents/GitHub/CST-425/Clustering/online_shoppers_intention.csv
kmeansdf <- read.csv("C:/Users/andre/Documents/GitHub/CST-425/Clustering/online_shoppers_intention.csv")
kmeansdf <- data.frame(kmeansdf)
head(kmeansdf)

```

### Association Rules Mining Data


# K-Means

## Step 2 (K-Means): Pre-processing Data

### Data Cleaning
### Normalizing Data for KMeans is useful since the algorithm calculates the different clusters based on distance
```{r KMeansDataCleaning}
# Data cleaning
#head(kmeansdf)
scalecolumns <- c(1,2,3,4,5,6,7,8,9,10,12,13,14,15) # picks integer columns we want to scale
kmeansdf <- kmeansdf[,scalecolumns]
head(kmeansdf)

print(sum(is.na(kmeansdf))) # Do not need to omit missing values because there are none
# kmeansdf <- na.omit(kmeansdf) # Get rid of missing values

kmeansdf <- scale(kmeansdf)

head(kmeansdf)

# Detect outliers using z-score method
z_scores <- as.data.frame(sapply(kmeansdf, function(kmeansdf) (abs(kmeansdf-mean(kmeansdf))/sd(kmeansdf))))

# Show rows in dataframe with all z-scores less than absolute value of 3
#no_outliers <- z_scores[!rowSums(z_scores>3), ]
```

## Step 3 (K-Means): Build the model

### Decide number of groups using the elbow method
```{r KMeansElbowMethod}
set.seed(123)
kmeansdf2 <- kmeansdf
kmeansdf <- kmeansdf[1:3000]
kmeansdf <- data.frame(kmeansdf)
fviz_nbclust(kmeansdf, kmeans, method = "wss")
```

### Build the model using optimal number of groups and show the results
```{r KMeansModel}
kmeansmodel <- kmeans(kmeansdf2, centers = 2, nstart = 25)
str(kmeansmodel)
```

## Step 4 (K-Means): Run the model and make predictions
```{r KMeansPredictions}
# TK
```

## Step 5 (K-Means): Display the results (quantitative and visual)
```{r KMeansResults}
# Quantitative Results
head(kmeansmodel)
head(kmeansdf)
# Visual Results
fviz_cluster(kmeansmodel, data = kmeansdf2)
```

## Steps 6-7 (K-Means): Interpret the results and adjust your clustering
For KMeans, the calculation is taken from a distance equation from each of the variables for each instance. This means that all non-integer values will need to be changed into integer values if possible or ignored. Another part of the cleaning process was normalizing the variables so that they are all scaled uniformly so that outliers do not cause as big of a disruption. From our first calculation of the elbow graph, the optimal grouping for our KMeans k value is 2. This will split the data into two different groups. kmeans() is used to calculate the kmean model while fvis_cluster() will be used to visualize the kmeans model.







#------------------------------------------------------------------------------------------------------


# Association Rules Mining

```{r getDataAssociation}
# Evan's CWD -> C:/Users/Evan/OneDrive/Documents/GitHub/CST-425/Clustering/Store_Hackathon_Ideal_Data.csv
# Andrew's CWD -> C:/Users/andre/Documents/GitHub/CST-425/Clustering/Store_Hackathon_Ideal_Data.csv
associationdf <- read.csv("C:/Users/andre/Documents/GitHub/CST-425/Clustering/data.csv")
associationdf <- associationdf[,c(1,3)]
# associationdf <- associationdf %>% relocate(BRD, .before = MONTH)   # puts brand as first column instead of last
associationdf <- as (associationdf, "transactions") # convert to 'transactions' class


str(associationdf)
head(associationdf)
```


## Step 2 (Association Rules Mining): Pre-processing Data

### Setup variables and remove unnecessary columns
```{r AssociationVariables}
head(associationdf)
#inspect(associationdf)
```

### Data Cleaning
```{r AssociationDataCleaning}
#associationdf[,1] <- as.factor(associationdf[,1])
#associationdf[,2] <- as.factor(associationdf[,2])
#associationdf[,3] <- as.factor(associationdf[,3])
#associationdf[,4] <- as.factor(associationdf[,4])
#associationdf[,5] <- as.factor(associationdf[,5])
#associationdf[,6] <- as.factor(associationdf[,6])
#associationdf[,7] <- as.factor(associationdf[,7])
#associationdf[,8] <- as.factor(associationdf[,8])
#associationdf[,9] <- as.factor(associationdf[,9])
#associationdf[,10] <- as.factor(associationdf[,10])

#class(associationdf)
#head(associationdf)
```

## Step 3 (Association Rules Mining): Build the model

### Decide lift and support using the elbow method
```{r AssociationElbowMethod}
#set.seed(123)
# TK
```

### Build the model using optimal variables and show the model results
```{r AssociationModel}

#TK -> Need to figure out if the xlabels being all weird is normal or if we have to reforma

frequentItems <- eclat(associationdf, parameter = list(supp = 0.07, maxlen = 15)) # calculates support for frequent items
inspect(frequentItems)
itemFrequencyPlot(associationdf, topN=20, type="absolute", main="Item Frequency") # plot frequent items
```

## Step 4 (Association Rules Mining): Run the model and make predictions
```{r AssociationPredictions}
rules <- apriori(associationdf, parameter = list(supp = 0.001, conf = 0.01)) # Min Support as 0.001, confidence as 0.5.
```

## Step 5 (Association Rules Mining): Display the results (quantitative and visual)
```{r AssociationResults}
# Quantitative Results
head(rules)

# Visual Results

```

```{r}
rules_conf <- sort(rules, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_conf)) # show the support, lift and confidence for all rules

rules_lift <- sort(rules, by="lift", decreasing=TRUE) # 'high-lift' rules.
inspect(head(rules_lift)) # show the support, lift and confidence for all rules
```

## Steps 6-7 (Association Rules Mining): Interpret the results and adjust your clustering
Explain here TK


# References
https://uc-r.github.io/kmeans_clustering#kmeans

https://www.datanovia.com/en/lessons/data-preparation-and-r-packages-for-cluster-analysis/

https://www.statology.org/remove-outliers-r/