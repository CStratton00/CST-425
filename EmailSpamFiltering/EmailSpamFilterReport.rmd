---
title: "Email Spam Filtering"
output: html_notebook
---

## Step 1: Data Setup and Exploration

### Setup Knitr
```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

### Get the appropriate libraries
```{r libraries}
library(tidyverse)
library(e1071)
library(mice)
library(Amelia)
library(scales)
library(ggplot2)
```

### Load the Data
```{r loadData}
# Get Data from ClassExamples Directory
spam <- read_csv(file='EmailSpamFiltering/completeSpamAssassin.csv')
head(spam,1)
```

# Explore the model by getting a glimpse of the data.
```{r exploreModel}
glimpse(spam)
```

TK

# Step 2: Improve the Bayesian Model

## Getting Rid of Missing Values
### Naive Bayes is handy due to laplace smoothing which can allow not having to clean data for missing values. Although this can be useful, it still is a good implementation to clean the data. In this example there are about 500 entries with just the email body being the string "empty" which the model could unintentionally pick up as a key word in determining spam/ham in the model. So in this case scenario we removed all "empty" body text instances. By removing all of the "empty" body text instances, we avoid the zero-probability problem for this model.

## Lowercase/Uppercase text
### The idea behind making all of the text lowercase would be to eliminate cases where words that are the same but have different capitalization are considered different. Example being (free and FREE). This would be beneficial if in the context of the word they mean the same, but in this case, a sign of a spam email would be words in all caps, meaning that it is important to keep the text as is to increase the accuracy of the model.

##Changing Training/Testing Set Ratios
### A way to change the accuracy of the model can be changing the ratio for the testing and training sets between 70:30 to 80:20.

```{r improveModel}
#------------------------------------Get rid of missing values---------------------------------------------------------
glimpse(spam)

#Change all "empty" spam emails to NA
spam[spam == "empty"] <- NA

#Count the amount of emails that have NA
sum(is.na(spam))

#Remove all email cases with NA in them
spam.removedNA <- na.omit(spam)

glimpse(spam.removedNA)

#-----------------------Lowercase/Upercase (Theoretical Implementation)-----------------------------------------
#Make all words lowercase to prevent multiple matching
# Lowercase the model
#spam$Body <- tolower(spam$Body)
#spam$Body <- toupper(spam$Body)


#-----------------Changing Testing and Training Set Ratios (Theoretical Implementation)---------------------------
# Create a sample of the data
#set.seed(1234)
#sample_set <- sample(1:dim(spam.removedNA)[1], dim(spam.removedNA)[1]*0.75)

#Split training and testing set using 75:25 split
#spam_train <- spam[sample_set,]
#spam_test <- spam[-sample_set,]

```

# Step 3: Preparing training and testing sets
```{r model}
# Set a seed to randomize model generation
set.seed(1234)

# Create a sample of the data
sample_set <- sample(1:dim(spam.removedNA)[1], dim(spam.removedNA)[1]*0.80)

# Split training and testing set using 80:20 split
spam_train <- spam.removedNA[sample_set,]
spam_test <- spam.removedNA[-sample_set,]
```

TK

## Step 4: Build the Bayesian Model (Naive)
```{r buildModel}
# Build the naive model based on training set
spam_mod <- naiveBayes(Label ~ ., data = spam_train, laplace = 1)
```

# Step 5: Interpret the model
```{r}
head(spam_mod,1)
```

TK (plus maybe interpret this more, if possible)

## Step 5: Make predictions using the test set
```{r predict}
spam_pred_raw <- predict(spam_mod, newdata = spam_test, type = "raw")
head(spam_pred_raw)
spam_pred_class <- predict(spam_mod, newdata = spam_test, type = "class")
head(spam_pred_class)
```

Using the improved prediction model for the spam email set, make a prediction about whether an email is spam or not using the testing set.
The first output shows the probability of an email being spam or not spam (separated by columns 0 and 1). The second output generates an array of prediction values stating if the email is spam (1) or not spam(0), based on which one is more accurate.


## Step 7: Final Spam Identification Results and Estimate the Error

### Create confusion matrix of the results and compute the model accuracy
```{r confusion}
# Print Confusion Matrix
spam_pred_table <- table(spam_test$Label, spam_pred_class)
spam_pred_table

# State the model accuracy
accuracy <- sum(diag(spam_pred_table)) / nrow(spam_test)
cat("Accuracy: ", accuracy)
error_rate <- 1-accuracy
cat("\nError Rate: ", error_rate)
```

The accuracy of our model is 99.2% with a seed of 1234. The confusion matrix shows a nearly perfect prediction accuracy for detecting actual non-spam emails with an 774/784, while the prediction of actual spam emails is perfect with an accuracy of 321. Overall, the testing set shows that the model has high accuracy in predicting spam emails.


## Step 8: Model Verification
```{r modelVerification}
# Ensure the seed is set to 1234
set.seed(1234)

# Get Data from ClassExamples Directory
spam_test2 <- read_csv(file='EmailSpamFiltering/lingSpam.csv')
head(spam_test2)
glimpse(spam_test2)

# Make a prediction for the lingSpam dataset
spam_pred2 <- predict(spam_mod, newdata = spam_test2, type = "class")
head(spam_pred2)
```

One method of verifying the model is to test it on another set of email data. Using the lingSpam.csv file (with the same column format), use the predict function to create an array of prediction values.

```{r seeVerificationResults}
# Print Confusion Matrix
spam_pred_table2 <- table(spam_test2$Label, spam_pred2)
spam_pred_table2

# State the prediction accuracy
accuracy <- sum(diag(spam_pred_table2)) / nrow(spam_test2)
cat("Accuracy: ", accuracy)
error_rate <- 1-accuracy
cat("\nError Rate: ", error_rate)
```

The accuracy of this prediction is 45.3% with a seed of 1234. The confusion matrix shows a perfect prediction accuracy for detecting actual non-spam emails. However, the prediction accuracy for actual spam emails is low with an accuracy of 433/1857. However, notice that the ratio of non-spam to spam emails in the model training set is about 35%/65%, whereas this model is the opposite with 40%/60%. This may result in a skew of the data due to the difference in the expected ratio.

Moreover, the words used in the first data set skew the results of testing the model on the new data set. For example, the word "subject" might be found to be a part of 75% of all non-spam email in the first data set, whereas the new data set could include that word in 30% of all non-spam emails. Consequently, if the word commonly appears in the new data set, it could skew the results to predict more false positives. To mitigate this issue, the model used must include a large testing sample from the new data set to accurately predict spam from both sets.

Thus, this method of verifying the model is inconclusive.

## References
https://www.kaggle.com/nitishabharathi/email-spam-dataset